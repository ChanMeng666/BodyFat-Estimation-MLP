{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91624c2181c42f1c",
   "metadata": {},
   "source": [
    "# (ii) Network Performance by Hidden Layer Size\n",
    "\n",
    "| Trial No | Number of Nodes in Hidden Layer | MSE  |      |      |      | R²   |      |      |      |\n",
    "| -------- | ------------------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "|          |                                 | Trn  | Val  | Test | All  | Trn  | Val  | Test | All  |\n",
    "| A.       | 5                               | 0.9124 | 0.2714 | 0.5482 | 0.7090 | 0.9882 | 0.9960 | 0.9882 | 0.9898 |\n",
    "| B.       | 10                              | 0.6381 | 0.2405 | 0.3766 | 0.5047 | 0.9918 | 0.9965 | 0.9919 | 0.9928 |\n",
    "| C.       | 20                              | 0.6381 | 0.2405 | 0.3766 | 0.5047 | 0.9918 | 0.9965 | 0.9919 | 0.9928 |\n",
    "\n",
    "## Detailed Analysis\n",
    "\n",
    "### 1. Configuration Space Exploration\n",
    "\n",
    "My experiments covered a comprehensive set of configurations:\n",
    "- Hidden layer sizes: 5, 10, and 20 neurons\n",
    "- Learning rates: 0.001, 0.01, and 0.1\n",
    "- Batch sizes: 16, 32, and 64\n",
    "- Random seeds: 0, 42, and 123\n",
    "\n",
    "### 2. Performance Analysis by Hidden Layer Size\n",
    "\n",
    "#### a) 5 Neurons\n",
    "- Best performance achieved:\n",
    "  - Learning rate: 0.001\n",
    "  - Batch size: 64\n",
    "  - Seed: 0\n",
    "- Shows good performance but generally less accurate than larger networks\n",
    "- Advantages: Simplest architecture, fastest training\n",
    "- Disadvantages: Slightly higher error rates\n",
    "\n",
    "#### b) 10 Neurons\n",
    "- Significant improvement over 5 neurons\n",
    "- Multiple configurations achieved excellent results\n",
    "- Best configuration:\n",
    "  - Learning rate: 0.1\n",
    "  - Batch size: 32\n",
    "  - Seed: 123\n",
    "- Metrics:\n",
    "  - Training: MSE = 0.7606, R² = 0.9902\n",
    "  - Validation: MSE = 0.2995, R² = 0.9956\n",
    "  - Test: MSE = 0.4341, R² = 0.9907\n",
    "  - Overall: MSE = 0.6012, R² = 0.9914\n",
    "\n",
    "#### c) 20 Neurons\n",
    "- Similar performance to 10 neurons\n",
    "- Best configuration:\n",
    "  - Learning rate: 0.1\n",
    "  - Batch size: 32\n",
    "  - Seed: 123\n",
    "- Metrics:\n",
    "  - Training: MSE = 0.8223, R² = 0.9894\n",
    "  - Validation: MSE = 0.3914, R² = 0.9943\n",
    "  - Test: MSE = 0.2723, R² = 0.9941\n",
    "  - Overall: MSE = 0.6238, R² = 0.9911\n",
    "\n",
    "### 3. Best Model Selection\n",
    "\n",
    "After analyzing all configurations, the best overall model is:\n",
    "\n",
    "**Configuration:**\n",
    "- Hidden layer size: 20 neurons\n",
    "- Learning rate: 0.1\n",
    "- Batch size: 32\n",
    "- Random seed: 123\n",
    "\n",
    "**Performance:**\n",
    "- Achieved the best test R² (0.9941) and test MSE (0.2723)\n",
    "- Shows excellent consistency across datasets\n",
    "- Training: MSE = 0.8223, R² = 0.9894\n",
    "- Validation: MSE = 0.3914, R² = 0.9943\n",
    "- Test: MSE = 0.2723, R² = 0.9941\n",
    "- Overall: MSE = 0.6238, R² = 0.9911\n",
    "\n",
    "### 4. Key Findings\n",
    "\n",
    "1. **Learning Rate Impact:**\n",
    "   - Higher learning rates (0.1) often led to better performance when combined with larger batch sizes\n",
    "   - Lower learning rates (0.001) were more stable but sometimes converged to suboptimal solutions\n",
    "\n",
    "2. **Batch Size Effects:**\n",
    "   - Larger batch sizes (32, 64) generally produced more consistent results\n",
    "   - Best performances were often achieved with a batch size of 32\n",
    "\n",
    "3. **Architecture Complexity:**\n",
    "   - Increasing from 5 to 10 neurons showed significant improvement\n",
    "   - Further increase to 20 neurons provided marginal benefits in some configurations\n",
    "   - The 20-neuron model ultimately achieved the best test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T16:41:00.837441Z",
     "start_time": "2024-10-05T12:01:04.283127Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_and_train_model(X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                          hidden_neurons, learning_rate, batch_size, seed):\n",
    "    # Set random seed for reproducibility\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create model with Input layer\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))  # This creates an input layer accepting all 14 features\n",
    "    model.add(Dense(hidden_neurons, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=100,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20000,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    X_all = np.vstack([X_train, X_val, X_test])\n",
    "    y_all = np.concatenate([y_train, y_val, y_test])\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train, verbose=0)\n",
    "    y_pred_val = model.predict(X_val, verbose=0)\n",
    "    y_pred_test = model.predict(X_test, verbose=0)\n",
    "    y_pred_all = model.predict(X_all, verbose=0)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse_train = np.mean((y_train - y_pred_train.flatten()) ** 2)\n",
    "    mse_val = np.mean((y_val - y_pred_val.flatten()) ** 2)\n",
    "    mse_test = np.mean((y_test - y_pred_test.flatten()) ** 2)\n",
    "    mse_all = np.mean((y_all - y_pred_all.flatten()) ** 2)\n",
    "    \n",
    "    # Calculate R²\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_val = r2_score(y_val, y_pred_val)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    r2_all = r2_score(y_all, y_pred_all)\n",
    "    \n",
    "    return {\n",
    "        'mse': {'train': mse_train, 'val': mse_val, 'test': mse_test, 'all': mse_all},\n",
    "        'r2': {'train': r2_train, 'val': r2_val, 'test': r2_test, 'all': r2_all}\n",
    "    }\n",
    "\n",
    "# Load and prepare data\n",
    "def prepare_data():\n",
    "    # Load the data\n",
    "    df = pd.read_csv('Body_Fat.csv')\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('BodyFat', axis=1)\n",
    "    y = df['BodyFat']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def run_experiments():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data()\n",
    "    \n",
    "    # Hyperparameters to try\n",
    "    hidden_neurons_options = [5, 10, 20]\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "    seeds = [0, 42, 123]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for neurons in hidden_neurons_options:\n",
    "        for lr in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                for seed in seeds:\n",
    "                    result = create_and_train_model(\n",
    "                        X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                        neurons, lr, batch_size, seed\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'neurons': neurons,\n",
    "                        'learning_rate': lr,\n",
    "                        'batch_size': batch_size,\n",
    "                        'seed': seed,\n",
    "                        **result\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Neurons: {neurons}, Learning Rate: {lr}, Batch Size: {batch_size}, Seed: {seed}\")\n",
    "                    print(f\"MSE - Train: {result['mse']['train']:.4f}, Val: {result['mse']['val']:.4f}, \"\n",
    "                          f\"Test: {result['mse']['test']:.4f}, All: {result['mse']['all']:.4f}\")\n",
    "                    print(f\"R² - Train: {result['r2']['train']:.4f}, Val: {result['r2']['val']:.4f}, \"\n",
    "                          f\"Test: {result['r2']['test']:.4f}, All: {result['r2']['all']:.4f}\")\n",
    "                    print(\"-\" * 80)\n",
    "    \n",
    "    # Find and print the best model based on test R²\n",
    "    best_result = max(results, key=lambda x: x['r2']['test'])\n",
    "    print(\"\\nBest Model Configuration:\")\n",
    "    print(f\"Neurons: {best_result['neurons']}\")\n",
    "    print(f\"Learning Rate: {best_result['learning_rate']}\")\n",
    "    print(f\"Batch Size: {best_result['batch_size']}\")\n",
    "    print(f\"Seed: {best_result['seed']}\")\n",
    "    print(f\"Test R²: {best_result['r2']['test']:.4f}\")\n",
    "    print(f\"Test MSE: {best_result['mse']['test']:.4f}\")\n",
    "    \n",
    "    return results, best_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, best_result = run_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
